{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "173f77b2-dbc4-45c6-9329-346d7b8d817e",
   "metadata": {},
   "source": [
    "# K-Means Clustering Unsupervised Model\n",
    "**Sites Used**\\\n",
    "[Scikit-Learn KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\\\n",
    "[Scikit-Learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)\\\n",
    "[NSL-KDD With KMeans](https://notebook.community/nickjmiller/KMeansNSL-KDD/kmeans)\\\n",
    "[KDD'99 Statistical Analysis](https://www.researchgate.net/publication/341077418_A_Statistical_Analysis_on_KDD_Cup%2799_Dataset_for_the_Network_Intrusion_Detection_System)\n",
    "\n",
    "**Basic Definition**\\\n",
    "The KMeans algorithm clusters data by trying to separate samples in n groups of equal variances, by minimizing a criterion known as 'inertia' or 'within-cluster sum-of-squares'. This is done by first randomly placing k centroids and then iteratively assigning data points to the nearest center (using Euclidean distance measure). The centroids are adjusted as each cluster grows. The assignment process is repeated until all the centroids converge to a maximum or the maximum number of iterations have reached. \n",
    "\n",
    "**K-Means for Anomaly Detection**\\\n",
    "After the clusters have been formed, the distances of each data point to its center can be calculated. Points that are from their cluster center are potentially anomalous because they don't fit well with any cluster. Anomalies can also be bunched together to form very small clusters. A threshold can then be set in place to determine if a cluster is too small, or a data point is too far. \n",
    "\n",
    "**K-Means for KDD'99**\\\n",
    "Because of the large quantity normal traffic ~25%, K-Means can group network traffic into clusters that may correspond to normal traffic and different attack types. We can assume that the data points that do not fit well into any large, well-defined clusters can be flagged as anomalous.\n",
    "\n",
    "**Limitations**\n",
    "- Choosing K: A very low K value might oversimplify the data, while too high values may produce too fine-grained clusters, making it difficult to distinguish actual anomalies from noise.\n",
    "- Cluster Shape Assumption: K-Means assumes clusters to be similarly sized and roughly spherical. Network traffic that strays from this will worsen performance.\n",
    "- Outliers: By definition any data points / clusters that stray too much are anomalous. Additionally, anomalies themselves can skew cluster centroids.\n",
    "- Extra Logic: Unlike other models, KMeans doesn't have an inbuilt prediction method, all it does is assign data points to clusters. Extra logic is required for determining data points are anomalous or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2043abda-1c87-4731-9470-2eb69712aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame shapes after processing stage: \n",
      "(493969, 15)\n",
      "(292261, 15)\n"
     ]
    }
   ],
   "source": [
    "# Import everything, define hyperparameters and features.\n",
    "# Load the preprocessed data and apply variance threshold to it.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.utils import check_random_state\n",
    "import joblib\n",
    "from common_stage import prepareData\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    # [Clustering Model]\n",
    "    'k_clusters': 40, # Cluster Amount\n",
    "    'max_iter': 100,  # Maximum Iterations\n",
    "    'variance_threshold': 0.02,  # Variance Threshold for Data Features\n",
    "    \n",
    "    # [Anomaly Detection]\n",
    "    'min_size': 45000,\n",
    "    'per_dist': 99,\n",
    "    'w_distance': 0.8, # Centroid Distance Weight for Anomaly Detection\n",
    "    'w_cluster': 0.3,  # Min Cluster Size Weight for Anomaly Detection\n",
    "    'score_threshold': 0.56 # Maximum anomaly score for a data point to be classified as an anomaly\n",
    "}\n",
    "\n",
    "# Features to be dropped\n",
    "dropped_features = [\n",
    "    'srv_count', 'srv_serror_rate', 'srv_rerror_rate',\n",
    "    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate'\n",
    "]\n",
    "\n",
    "# Load and preprocess data\n",
    "x_train, y_train, x_test, y_test = prepareData(scaling='minmax', removed_features=dropped_features)\n",
    "\n",
    "# Apply variance threshold to remove low-variance features\n",
    "selector = VarianceThreshold(hyperparams['variance_threshold'])\n",
    "x_train = pd.DataFrame(selector.fit_transform(x_train), columns=x_train.columns[selector.get_support()])\n",
    "x_test = pd.DataFrame(selector.transform(x_test), columns=x_test.columns[selector.get_support()])\n",
    "\n",
    "print(\"Data frame shapes after processing stage: \")\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294c704-8620-48fc-bffc-c6a3e8a81024",
   "metadata": {},
   "source": [
    "---\n",
    "### Basic Grid Search for KMeans Hyperparameters (NEEDS UPDATING)\n",
    "```python\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Grid Search Parameters\n",
    "param_grid = {\n",
    "    'n_clusters': [4, 20, 100, 400, 800],\n",
    "    'max_iter': [100, 200, 300, 600]\n",
    "}\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "print(\"Now starting grid search...\")\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    # Initialize and fit the KMeans model with the current hyperparameters\n",
    "    kmeans = KMeans(n_clusters=params['n_clusters'], max_iter=params['max_iter'], random_state=42)\n",
    "    kmeans.fit(x_train)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    try:\n",
    "        # Compute the silhouette score using x_train (instead of the undefined x_cluster)\n",
    "        sil_score = silhouette_score(x_train, labels, sample_size=1000, random_state=42)\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            best_params = params\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Silhouette Score:\", best_score)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497cb82b-7e58-4a2d-b2d8-03f860a4a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train K-Means on training data\n",
    "\n",
    "kmeans = KMeans(n_clusters=hyperparams['k_clusters'], max_iter=hyperparams['max_iter'], random_state=42).fit(x_train)\n",
    "x_train['cluster'] = kmeans.labels_\n",
    "x_test['cluster'] = kmeans.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84fc75-5b78-48ab-a68f-73b132cc9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a plot of the clusters in 3D using PCA\n",
    "\n",
    "features = x_test.drop(columns=['cluster'])  # Keep it as a DataFrame\n",
    "pca = PCA(n_components=3)\n",
    "X_pca_test = pca.fit_transform(features)\n",
    "\n",
    "# Transform cluster centers as a DataFrame with the same columns\n",
    "centers_df = pd.DataFrame(kmeans.cluster_centers_, columns=features.columns)\n",
    "centers_pca = pca.transform(centers_df)\n",
    "# 3D cluster plot for x_test\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_pca_test[:, 0], X_pca_test[:, 1], X_pca_test[:, 2], c=x_test['cluster'], cmap='rainbow', alpha=0.6)\n",
    "ax.scatter(centers_pca[:, 0], centers_pca[:, 1], centers_pca[:, 2], c='black', marker='X', s=200, label='Cluster Centers')\n",
    "ax.set_xlabel(\"PCA Component 1\")\n",
    "ax.set_ylabel(\"PCA Component 2\")\n",
    "ax.set_zlabel(\"PCA Component 3\")\n",
    "ax.set_title(\"K-Means Clustering on x_test\")\n",
    "plt.legend()\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac12da-67f8-49af-af26-11a84d102f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Anomaly Detection Using Both Centroid Distances and Cluster Sizes\n",
    "# For each data point, the nearest centroid and the size its cluster are used to give an anomaly score\n",
    "# Threshold values are used for the distance, size, and anomaly score so that the binary classification of anomalies is as effective as possible.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "distances = kmeans.transform(features) # kmeans.transform returns the distance of each point to each cluster center.\n",
    "assigned_clusters = np.argmin(distances, axis=1) # Maps each data point to cluster based on the closest centroid\n",
    "\n",
    "percentile_threshold = np.percentile(assigned_clusters, hyperparams['per_dist']) # Get the threshold value.\n",
    "distance_component = assigned_clusters / percentile_threshold\n",
    "\n",
    "cluster_sizes = np.bincount(assigned_clusters)   # Compute cluster sizes using a vectorized approach\n",
    "cluster_sizes_per_point = np.array([cluster_sizes[cluster] for cluster in assigned_clusters]) # Map each data point to its cluster size\n",
    "\n",
    "# If a data point belongs to a cluster smaller than 'min_size', -\n",
    "# the cluster_component term is positive (0-1). Otherwise, it is 0.\n",
    "cluster_component = np.maximum(0, hyperparams['min_size'] - cluster_sizes_per_point) / hyperparams['min_size']\n",
    "\n",
    "# Get the final anomaly score by adding the components multiplied by their respective weights.\n",
    "anomaly_score = hyperparams['w_distance'] * distance_component + hyperparams['w_cluster'] * cluster_component\n",
    "\n",
    "score_threshold = hyperparams.get('score_threshold', 1.0)\n",
    "x_test['anomaly_score'] = anomaly_score\n",
    "x_test['anomaly'] = (anomaly_score > score_threshold).astype(int)  # Anomaly_score in binary by using score_threshold.\n",
    "\n",
    "print(\"Anomaly scoring computed. 'anomaly' and 'anomaly_score' columns added to x_test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206dbc14-cae6-4b04-bfd6-cc4370495fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the cluster size distribution\n",
    "\n",
    "# Create a bar chart of cluster sizes\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(cluster_sizes)), cluster_sizes, color='blue', alpha=0.7, edgecolor='black')\n",
    "min_size = hyperparams['min_size']\n",
    "plt.axhline(y=min_size, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f'min_size: {min_size}')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Cluster Size')\n",
    "plt.title('Cluster Size Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57104884-ed7b-4d58-a079-318161fb6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisze the distance distribution\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(assigned_clusters, bins=50, alpha=0.7)\n",
    "plt.axvline(percentile_threshold, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f'Percentile Threshold: {percentile_threshold:.2f}')\n",
    "plt.title('Distribution of Minimum Distances to Cluster Centers')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511c0ac-60f7-4bf3-afef-e8b22cd0b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the distribution of anomaly scores\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(anomaly_score, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Anomaly Scores')\n",
    "\n",
    "plt.axvline(x=score_threshold, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f'Score Threshold: {score_threshold:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafa6f5-fa92-4dd5-beae-f5b40a863218",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get evaluation metrics specific for clustering.\n",
    "# - Davies-Bouldin: Measures models performance by comparing the average similarity between the pairwise most similar clusters. Lower DBI is better.\n",
    "# - Calinski-Harabasz Index: Compares the ratio of the between-cluster dispersion with the within-cluster dispersion. Higher CI is better.\n",
    "# - Silhouette Score: Measues how well each data point is clustered by assessing its similarity to its own cluster compated to other clusters (the average).\n",
    "#    Close to +1 indicates that the datapoint is well defined to its cluster. Negative means that the data point has been assigned to the wrong cluster.\n",
    "#    And close to 0 suggets that the data point is close to the descision boundary of two clusters. It is not well defined.\n",
    "\n",
    "sample_size = 5000\n",
    "random_state = check_random_state(42)\n",
    "sil_score = silhouette_score(x_test, x_test['cluster'], sample_size=sample_size, random_state=random_state)\n",
    "dbi_score = davies_bouldin_score(x_test, x_test['cluster'])\n",
    "ch_score = calinski_harabasz_score(x_test, x_test['cluster'])\n",
    "\n",
    "print(\"Silhouette Score:\", sil_score)\n",
    "print(\"Davies-Bouldin Index:\", dbi_score)\n",
    "print(\"Calinski-Harabasz Index:\", ch_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdecbb8-eeaa-43cb-9207-a3bf42c3a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evalutate Anomaly Detection\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score,  roc_auc_score, accuracy_score\n",
    "\n",
    "# Convert multi-class labels to binary (0: Normal, 1: Attack)\n",
    "true_binary_labels = np.where(y_test == 0, 0, 1)\n",
    "\n",
    "# Ensure predictions are aligned\n",
    "predicted_anomalies = x_test['anomaly'].reset_index(drop=True)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_binary_labels, predicted_anomalies, target_names=[\"Normal\", \"Attack\"], zero_division=0))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(true_binary_labels, predicted_anomalies)\n",
    "print(cm)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(true_binary_labels, predicted_anomalies, zero_division=0)\n",
    "recall = recall_score(true_binary_labels, predicted_anomalies, zero_division=0)\n",
    "f1 = f1_score(true_binary_labels, predicted_anomalies, zero_division=0)\n",
    "accuracy = accuracy_score(true_binary_labels, predicted_anomalies)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "try:\n",
    "    roc_auc = roc_auc_score(true_binary_labels, predicted_anomalies)\n",
    "    print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"Couldn't compute ROC AUC\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1044f8c-2b59-4f73-874a-595f1b83ce13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
